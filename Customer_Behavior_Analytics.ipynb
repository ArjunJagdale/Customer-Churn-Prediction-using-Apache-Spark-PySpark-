{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üìä Customer Churn Prediction using Apache Spark (PySpark)\n",
        "\n",
        "This project demonstrates an **end-to-end machine learning pipeline** built using **Apache Spark (PySpark)**.  \n",
        "We analyze customer behavior data to predict churn using a logistic regression model, combined with data cleaning, feature engineering, model tuning, and exporting final results.\n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ Project Highlights\n",
        "\n",
        "- Built entirely with **Apache Spark via PySpark**\n",
        "- Handles **large-scale tabular data**\n",
        "- End-to-end ML pipeline using **Spark MLlib**\n",
        "- Includes:\n",
        "  - Data cleaning\n",
        "  - Feature engineering (indexing + assembling)\n",
        "  - Model training using logistic regression\n",
        "  - Cross-validation with hyperparameter tuning\n",
        "  - Churn rate analysis by contract type\n",
        "  - Saving outputs and model artifacts\n",
        "\n",
        "---\n",
        "\n",
        "## üõ†Ô∏è Technologies Used\n",
        "\n",
        "- Apache Spark 3.x\n",
        "- PySpark (DataFrame API + MLlib)\n",
        "- Logistic Regression (binary classification)\n",
        "- CrossValidator for model tuning\n",
        "- Pandas/CSV output compatible with cloud or local processing\n",
        "\n",
        "---\n",
        "\n",
        "## üìÅ Folder Structure / Output Explanation\n",
        "\n",
        "After running the notebook, you will see two output folders:\n",
        "\n",
        "### `/output/predictions/`\n",
        "- Contains a file like:  \n",
        "  `part-00000-<uuid>.csv`\n",
        "- **Includes:** model predictions and churn probabilities for each customer.\n",
        "- Columns:\n",
        "  - `customerID`: Unique identifier\n",
        "  - `prediction`: Churn prediction (0 = No, 1 = Yes)\n",
        "  - `churn_probability`: Probability of churn (from logistic regression)\n",
        "\n",
        "### `/output/churn_rate_by_contract/`\n",
        "- Contains a file like:  \n",
        "  `part-00000-<uuid>.csv`\n",
        "- **Includes:** churn rate aggregated by contract type.\n",
        "- Helps answer: *Which customer contracts have the highest churn rate?*\n",
        "\n",
        "---\n",
        "\n",
        "## üì¶ How to Run\n",
        "\n",
        "### üîπ Option 1: On Google Colab (Recommended for Beginners)\n",
        "\n",
        "1. Upload this notebook to Colab\n",
        "2. Upload your dataset (e.g., `customer_churn.csv`)\n",
        "3. Install PySpark:\n",
        "   ```bash\n",
        "   !pip install pyspark\n"
      ],
      "metadata": {
        "id": "BjtuRh13THsy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, count, avg, when, isnan\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import lag, datediff"
      ],
      "metadata": {
        "id": "Sot-TdWKTKkx"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Initialize Spark Session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"CustomerBehaviorAnalytics\") \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "exiTG7K-TK6S"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Load Dataset (CSV Format)\n",
        "df = spark.read.csv(\"/content/Telco-Customer-Churn.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Step 3: Data Cleaning\n",
        "cleaned_df = df.dropna()\n",
        "\n",
        "# Step 4: Basic Data Exploration\n",
        "cleaned_df.printSchema()\n",
        "cleaned_df.select(\"gender\", \"SeniorCitizen\", \"MonthlyCharges\").show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfaH6ZXATMga",
        "outputId": "589c34f2-0718-49b4-8e28-fd8eeaa9830c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- customerID: string (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- SeniorCitizen: integer (nullable = true)\n",
            " |-- Partner: string (nullable = true)\n",
            " |-- Dependents: string (nullable = true)\n",
            " |-- tenure: integer (nullable = true)\n",
            " |-- PhoneService: string (nullable = true)\n",
            " |-- MultipleLines: string (nullable = true)\n",
            " |-- InternetService: string (nullable = true)\n",
            " |-- OnlineSecurity: string (nullable = true)\n",
            " |-- OnlineBackup: string (nullable = true)\n",
            " |-- DeviceProtection: string (nullable = true)\n",
            " |-- TechSupport: string (nullable = true)\n",
            " |-- StreamingTV: string (nullable = true)\n",
            " |-- StreamingMovies: string (nullable = true)\n",
            " |-- Contract: string (nullable = true)\n",
            " |-- PaperlessBilling: string (nullable = true)\n",
            " |-- PaymentMethod: string (nullable = true)\n",
            " |-- MonthlyCharges: double (nullable = true)\n",
            " |-- TotalCharges: string (nullable = true)\n",
            " |-- Churn: string (nullable = true)\n",
            "\n",
            "+------+-------------+--------------+\n",
            "|gender|SeniorCitizen|MonthlyCharges|\n",
            "+------+-------------+--------------+\n",
            "|Female|            0|         29.85|\n",
            "|  Male|            0|         56.95|\n",
            "|  Male|            0|         53.85|\n",
            "|  Male|            0|          42.3|\n",
            "|Female|            0|          70.7|\n",
            "+------+-------------+--------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Feature Engineering\n",
        "processed_df = cleaned_df.withColumn(\"ChurnFlag\", when(col(\"Churn\") == \"Yes\", 1).otherwise(0))\n",
        "\n",
        "# Encode categorical features\n",
        "indexers = [\n",
        "    StringIndexer(inputCol=column, outputCol=column+\"_Index\")\n",
        "    for column in [\"gender\", \"InternetService\", \"Contract\", \"PaymentMethod\"]\n",
        "]\n",
        "\n",
        "for indexer in indexers:\n",
        "    processed_df = indexer.fit(processed_df).transform(processed_df)\n",
        "\n",
        "# Assemble features\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=[\"MonthlyCharges\", \"tenure\", \"gender_Index\", \"InternetService_Index\", \"Contract_Index\", \"PaymentMethod_Index\"],\n",
        "    outputCol=\"features\"\n",
        ")\n",
        "assembled_df = assembler.transform(processed_df)"
      ],
      "metadata": {
        "id": "JCkTkmP1TO9s"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: MLlib Logistic Regression Model\n",
        "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"ChurnFlag\")\n",
        "model = lr.fit(assembled_df)\n",
        "predictions = model.transform(assembled_df)"
      ],
      "metadata": {
        "id": "-M3sQCCbTVUH"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Evaluation\n",
        "evaluator = BinaryClassificationEvaluator(labelCol=\"ChurnFlag\")\n",
        "auc = evaluator.evaluate(predictions)\n",
        "print(f\"AUC: {auc:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-vu6xMxTXqK",
        "outputId": "36967bdd-fc84-448f-97ac-4ca8760855d7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC: 0.83\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8: Cross Validation with Param Grid\n",
        "grid = ParamGridBuilder().addGrid(lr.regParam, [0.01, 0.1, 1.0]).build()\n",
        "cv = CrossValidator(estimator=lr, estimatorParamMaps=grid, evaluator=evaluator, numFolds=3)\n",
        "cv_model = cv.fit(assembled_df)\n",
        "cv_predictions = cv_model.transform(assembled_df)\n",
        "cv_auc = evaluator.evaluate(cv_predictions)\n",
        "print(f\"Cross-Validated AUC: {cv_auc:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1nu3x8czTZR5",
        "outputId": "c244296f-a7f2-43cc-bd9a-0d8587a9dc4d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-Validated AUC: 0.83\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 9: Aggregation - Avg Monthly Charges by Internet Service\n",
        "agg_df = processed_df.groupBy(\"InternetService\") \\\n",
        "    .agg(avg(\"MonthlyCharges\").alias(\"AvgMonthlyCharge\"))\n",
        "agg_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ipf9SuVVTbb8",
        "outputId": "d9585b48-6c83-4241-9875-f6ff1e3faa62"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+------------------+\n",
            "|InternetService|  AvgMonthlyCharge|\n",
            "+---------------+------------------+\n",
            "|    Fiber optic| 91.50012919896615|\n",
            "|             No|21.079193971166454|\n",
            "|            DSL| 58.10216852540261|\n",
            "+---------------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 10: Churn Rate by Contract Type\n",
        "churn_rate = processed_df.groupBy(\"Contract\") \\\n",
        "    .agg(avg(\"ChurnFlag\").alias(\"ChurnRate\"))\n",
        "churn_rate.orderBy(\"ChurnRate\", ascending=False).show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4QysXzzITdYA",
        "outputId": "0a22376b-4074-46ec-89f4-a2ab8e093846"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+-------------------+\n",
            "|      Contract|          ChurnRate|\n",
            "+--------------+-------------------+\n",
            "|Month-to-month| 0.4270967741935484|\n",
            "|      One year|0.11269517990495587|\n",
            "|      Two year|0.02831858407079646|\n",
            "+--------------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.ml.linalg import VectorUDT\n",
        "from pyspark.sql.types import DoubleType\n",
        "\n",
        "# UDF to extract prob[1]\n",
        "extract_prob = udf(lambda v: float(v[1]), DoubleType())\n",
        "\n",
        "predictions_with_score = predictions.withColumn(\"churn_probability\", extract_prob(col(\"probability\")))\n",
        "\n",
        "predictions_with_score.select(\"customerID\", \"prediction\", \"churn_probability\") \\\n",
        "    .write.csv(\"output/predictions\", header=True)\n"
      ],
      "metadata": {
        "id": "CdgR5FtVThjt"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stop Spark\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "PYT3pQ0cTjhx"
      },
      "execution_count": 28,
      "outputs": []
    }
  ]
}